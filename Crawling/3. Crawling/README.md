# 크롤링
## [웹 크롤링 권한 확인(robots.txt)](https://searchadvisor.naver.com/guide/seo-basic-robots)
> 웹사이트에서 크롤링하며 정보를 수집하는 검색엔진 크롤러가 엣세스 하거나 정보수집을 해도 되는 페이지가 무엇인지, 해서는 안 되는 페이지가 무엇인지 알려주는 역할을 하는 파일입니다.   
- robots.txt를 사용하는 이유
    - 검색엔진 크롤러의 과도한 크롤링 및 요청으로 인한 과부하 방지를 위해
    - 검색엔진 크롤러의 일일 요청 수를 뜻하는 크롤 버짓 장비 방지를 위해
    - 검색엔진 크롤러에게 사이트맴의 위치를 제공하여 웹사이트의 콘텐츠가 검색엔진에게 더 잘 발견될 수 있도록 하기 위해

# RPA(Robotic Process Automation; 업무 자동화)
> RPA란, '로봇을 이용한 프로세스의 자동화'로 직역할 수 있습니다. 해석을 한다면, S/W를 이용하여 반복적으로 일어나는 업무 프로세스를 자동화하는 것을 의미하고 있습니다. 요즘에는 많은 사람들이 컴퓨터를 통해서 업무를 수행하고 있으며, 이중 반복적으로 일어나는 업무들은 대부분 RPA를 이용하여 자동화를 할 수 있습니다.
### RPA를 적용하기에 알맞은 업무 특성
- 정해진 룰 기반으로 진행됨
- 반복적이고 양이 많음
- 처리 방법이 변하지 않음
- 빠른 대응이 필요함
- 업무 처리 과정 / 결과의 기록이 요구됨

### 관련 기술
- [pyautogui](https://pyautogui.readthedocs.io/en/latest/)
> 마우스/키보드 제어 패키지
- [requests](https://www.w3schools.com/python/module_requests.asp)
> HTTP 요청을 보내는 패키지
- [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
> HTML, XML, JSON 등 파일의 구문을 분석하는 패키지
- [selenium](https://www.selenium.dev/documentation/webdriver/)
- [scrapy](https://docs.scrapy.org/en/latest/intro/tutorial.html)
- [openpyxl](https://openpyxl.readthedocs.io/en/stable/)
> - https://m.hanbit.co.kr/channel/category/category_view.html?cms_code=CMS9733780399


### 참고
- https://library.gabia.com/contents/9239/
- https://celltong.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%81%AC%EB%A1%A4%EB%A7%81
- https://nadocoding.tistory.com/15
- https://wikidocs.net/85383


- https://pythonblog.co.kr/coding/7/